{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n"
     ]
    }
   ],
   "source": [
    "pokemon = []\n",
    "path = \"pokemon/global-link/\"\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.png'):\n",
    "        if file.startswith('.'):\n",
    "            continue\n",
    "        pic_path = path + str(file)                                                                        \n",
    "        im = scipy.misc.imread(pic_path, flatten=False, mode='RGB')\n",
    "        im = scipy.misc.imresize(im,(72,72))\n",
    "        \n",
    "        image = np.asarray(im)\n",
    "        pokemon.append(image)\n",
    "print(len(pokemon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    random.shuffle(data)\n",
    "    for index in range(0, len(data), batch_size):\n",
    "        yield data[index: index + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "\n",
    "    real_input = tf.placeholder(tf.float32, [None, image_width, image_height, image_channels])\n",
    "    Z = tf.placeholder(tf.float32, [None, z_dim])\n",
    "\n",
    "    return real_input, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z,reuse=False, alpha=0.1,epsilon=1e-3):\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    " \n",
    "        batch_size = tf.shape(z)[0]\n",
    "        w_ = tf.get_variable('gener_a_', [100, 325*9*9])\n",
    "        w0 = tf.get_variable('gener_a', [3, 3, 512, 9*9*13], initializer = tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "        w1 = tf.get_variable('gener_a1', [3, 3, 256, 512], initializer = tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "        w2 = tf.get_variable('gener_a2', [3, 3, 128, 256], initializer = tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "        w3 = tf.get_variable('gener_a3', [3, 3, 3, 128], initializer = tf.contrib.layers.xavier_initializer(), dtype=tf.float32)     \n",
    "        b1 = tf.get_variable('b1', [512], initializer = tf.zeros_initializer())\n",
    "        b2 = tf.get_variable('b2', [256], initializer = tf.zeros_initializer())\n",
    "        b3 = tf.get_variable('b3', [128], initializer = tf.zeros_initializer())\n",
    "        b4 = tf.get_variable('b4', [3], initializer = tf.zeros_initializer())\n",
    "\n",
    "\n",
    "        scale1 = tf.get_variable('sc1', [512], initializer= tf.ones_initializer())\n",
    "        beta1 = tf.get_variable('bet1', [512], initializer=tf.zeros_initializer())\n",
    "        scale2 = tf.get_variable('sc2', [256], initializer= tf.ones_initializer())\n",
    "        beta2 = tf.get_variable('bet2', [256], initializer=tf.zeros_initializer())\n",
    "        scale3 = tf.get_variable('sc3', [128], initializer= tf.ones_initializer())\n",
    "        beta3 = tf.get_variable('bet3', [128], initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "        h0 = tf.matmul(z, w_)\n",
    "        h0 = tf.reshape(h0, [batch_size, 5, 5, 9*9*13])\n",
    "\n",
    "        h1 = tf.nn.conv2d_transpose(h0, w0, [batch_size, 9, 9, 512], strides=[1,2,2,1], padding='SAME')\n",
    "        h1 = tf.add(h1, b1)\n",
    "        batch_mean1, batch_var1 = tf.nn.moments(h1,[0])\n",
    "        h1 = tf.nn.batch_normalization(h1, batch_mean1, batch_var1, beta1, scale1, epsilon)\n",
    "        h1 = tf.nn.leaky_relu(h1, alpha) \n",
    "\n",
    "        h2 = tf.nn.conv2d_transpose(h1, w1, [batch_size, 18, 18, 256], strides=[1,2,2,1], padding='SAME' )\n",
    "        h2 = tf.add(h2, b2)\n",
    "        batch_mean2, batch_var2= tf.nn.moments(h2,[0])\n",
    "        h2 = tf.nn.batch_normalization(h2, batch_mean2, batch_var2, beta2, scale2, epsilon)\n",
    "        h2 = tf.nn.leaky_relu(h2, alpha) \n",
    "\n",
    "        h3 = tf.nn.conv2d_transpose(h2, w2, [batch_size, 36, 36, 128], strides=[1,2,2,1], padding='SAME')\n",
    "        h3 = tf.add(h3, b3)\n",
    "        batch_mean3, batch_var3 = tf.nn.moments(h3,[0])\n",
    "        h3 = tf.nn.batch_normalization(h3, batch_mean3, batch_var3, beta3, scale3, epsilon)            \n",
    "        h3 = tf.nn.leaky_relu(h3, alpha)  \n",
    "\n",
    "        logits = tf.nn.conv2d_transpose(h3, w3, [batch_size, 72, 72, 3], strides=[1,2,2,1], padding='SAME')       \n",
    "        logits = tf.add(logits, b4)\n",
    "\n",
    "        out = tf.tanh(logits)\n",
    "\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(images, reuse=False, alpha=0.1, dropout=0.5):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        h1 = tf.layers.conv2d(images, 128, 3, strides=1, padding='SAME', kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.zeros_initializer())\n",
    "        h1 = tf.nn.avg_pool(h1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        h1 = tf.nn.dropout(h1, dropout)\n",
    "        h1 = tf.nn.leaky_relu(h1, alpha)\n",
    "        \n",
    "        h2 = tf.layers.conv2d(h1, 256, 3, strides=1, padding='SAME' , kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.zeros_initializer())\n",
    "        h1 = tf.nn.avg_pool(h2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        h2 = tf.nn.leaky_relu(h2, alpha)\n",
    "        \n",
    "        h3 = tf.layers.conv2d(h2, 512, 3, strides=1, padding='SAME' , kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.zeros_initializer())\n",
    "        h3 = tf.nn.avg_pool(h3, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        h3 = tf.nn.leaky_relu(h3, alpha)\n",
    "        \n",
    "        flat = tf.reshape(h3, (-1, 9*9*512))\n",
    "        flat = tf.nn.dropout(flat, 0.5)\n",
    "        \n",
    "        logits = tf.layers.dense(flat, 1 , kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.zeros_initializer())\n",
    "        out = tf.sigmoid(logits)\n",
    "\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done..\n"
     ]
    }
   ],
   "source": [
    "input_size = pokemon[0].shape[0] * pokemon[0].shape[1]\n",
    "z_dim = 100\n",
    "out_channel_dim = 3\n",
    "h_dim = 72\n",
    "w_dim = 72\n",
    "smoothing = 0.1\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "print(\"done..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "input_real, input_z = model_inputs(w_dim, h_dim, out_channel_dim, z_dim)\n",
    "g_model, g_logits = generator(input_z)\n",
    "d_model_real, d_logits_real = discriminator(input_real)\n",
    "d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
    "tf.summary.image('output_image',g_model)\n",
    "print(\"done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_diff = 0\n",
    "var_diff = 0\n",
    "d_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_model_real)*(1-smoothing))) \n",
    "d_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake)))\n",
    "g_loss_mod = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake)))\n",
    "\n",
    "g_loss = g_loss_mod + mean_diff + var_diff\n",
    "d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discrim')]\n",
    "g_vars = [var for var in t_vars if var.name.startswith('gene')]\n",
    "\n",
    "#with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): \n",
    "g_train_opt = tf.train.AdamOptimizer(0.0002).minimize(g_loss, var_list=g_vars)\n",
    "d_train_opt = tf.train.AdamOptimizer(0.0001).minimize(d_loss, var_list=d_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100)\n",
      "(32, 100)\n",
      "(32, 100)\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "losses = []\n",
    "var_losses = []\n",
    "mean_losses = []\n",
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    graph_def = sess.graph\n",
    "    tf_tensorboard = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(\"./checkpoints/train_store_conv_lstm\",graph_def)\n",
    "    summary_writer_it = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for ii in range(len(pokemon)//batch_size):\n",
    "            poke_batch = next(batch(pokemon, batch_size))\n",
    "            poke_batch = ((np.array(poke_batch) / 255.) - 0.5) * 2\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "            print(batch_z.shape)\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_real: poke_batch, input_z: batch_z})\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real: poke_batch, input_z: batch_z})\n",
    "            summary = sess.run(tf_tensorboard, feed_dict={input_real: poke_batch, input_z: batch_z})\n",
    "            summary_writer.add_summary(summary, summary_writer_it)\n",
    "            summary_writer_it += 1\n",
    "\n",
    "        \n",
    "        train_loss_g = g_loss_mod.eval({input_z: batch_z, input_real: poke_batch})\n",
    "        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: poke_batch})\n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g)) \n",
    "        #var_diff1 = var_diff.eval(session=sess, feed_dict={input_real: poke_batch, input_z: batch_z})\n",
    "        #mean_diff1 = mean_diff.eval(session=sess, feed_dict={input_real: poke_batch, input_z: batch_z})\n",
    "        #print('Var_diff: ' +str(var_diff1) + ' Mean_diff: ' +str(mean_diff1))\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        #var_losses.append(var_diff1)\n",
    "        #mean_losses.append(mean_diff1)\n",
    "        sample_z = np.random.uniform(-1, 1, size=(32, z_dim))\n",
    "        gen_samples, _ = sess.run(\n",
    "                       generator(input_z, reuse=True),\n",
    "                       feed_dict={input_z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
